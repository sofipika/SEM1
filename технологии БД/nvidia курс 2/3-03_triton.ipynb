{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./images/DLI_Header.png\"></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton for Recommender Systems\n",
    "\n",
    "The [Triton Inference Server](https://github.com/triton-inference-server/server/blob/main/README.md#documentation) allows us to deploy our model to the web regardless of cloud provider, and it supports a number of different machine learning frameworks such as TensorFlow and PyTorch.\n",
    "\n",
    "## Objectives\n",
    "* Learn how to deploy a model to Triton\n",
    "  * [1. Deploy TensorFlow Model to Triton Inference Server](#1.-Deploy-TensorFlow-Model-to-Triton-Inference-Server)\n",
    "      * [1.1 Export a Model](#1.1-Export-a-Model)\n",
    "      * [1.2 Review exported files](#1.2-Review-exported-files)\n",
    "      * [1.3 Loading a Model](#1.3-Loading-a-Model)\n",
    "  * [2. Sent requests for predictions](#2.-Sent-requests-for-predictions)\n",
    "* Learn how to record deployment metrics\n",
    "  * [3. Server Metrics](#3.-Server-Metrics)\n",
    "\n",
    "## 1. Deploy TensorFlow Model to Triton Inference Server\n",
    "\n",
    "Our Triton server has already been launched to the web and is ready to make requests. First, we need to export the saved TensorFlow model from Lab 2 and generate the config file for Triton Inference Server. NVTabular provides an easy-to-use function, which manages both tasks.\n",
    "\n",
    "### 1.1 Export a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External dependencies\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "import tritonhttpclient\n",
    "\n",
    "import cudf\n",
    "import tritonclient.grpc as grpcclient\n",
    "import nvtabular.inference.triton as nvt_triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unzip the model that we saved as a zip file in the previous notebook, and then load it to be able to use it in the NVTabular `export_tensorflow_model()` function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip data/task_2_model.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load the TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model('task2_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will need the output name of the last layer to make predictions later, let's print them out using `model.output_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.output_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can export the model to `model_repository`. This folder is shared between the docker container for the jupyter notebook and the docker container that runs Triton Inference Server. Therefore, Triton will have access to the model files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvtabular\n",
    "\n",
    "# generate the TF saved model\n",
    "from nvtabular.inference.triton import export_tensorflow_model\n",
    "\n",
    "tf_config = export_tensorflow_model(model, \"wnd_tf\", \"model_repository/wnd_tf\", version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To free GPU memory, we will restart the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Review exported files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the files `export_tensorflow_model` created. Triton expects [a specific directory structure](https://github.com/triton-inference-server/server/blob/main/docs/model_repository.md) for our models. The folder `/model_repository` is shared with our server, and it expects the following format:\n",
    "\n",
    "```<model_repository_path>/\n",
    "  <model-name>/\n",
    "    [config.pbtxt]\n",
    "    <version-name>/\n",
    "      [model.savedmodel]/\n",
    "        <tensorflow_saved_model_files>/\n",
    "          ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree model_repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the generated config file. It defines the input columns with datatype and dimensions and the output layer. Manually creating this config file can be complicated and NVTabular provides an easy function with `export_tensorflow_model` to deploy TensorFlow model to Triton.\n",
    "\n",
    "Triton needs a [config file](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md) to understand how to interpret the model. Our `export_tensorflow_model` method is automaticall creating the config file and the required folder structure for us, so that we do not need to create it manually.\n",
    "\n",
    "The config file needs the following information:\n",
    "* name: The name of our model. Must be the same name as the parent folder.\n",
    "* platform: The type of framework serving the model.\n",
    "* input: The input our model expects.\n",
    "  * `name`: Should correspond with the model input name.\n",
    "  * `data_type`: Should correspond to the input's data type.\n",
    "  * `dims`: The dimensions of the *request* for the input, as in the dimensions of the data the user passes to us.\n",
    "  * `reshape`: How to reshape the data from the client to pass it to our model. In this case, the minimum dims from the client is `[1]`, but like Keras, Triton appends a dimension for batching. If our model expects `[batch_size]` as a dimension, we can reshape our data to `[]` (empty brackets) to account for that.\n",
    "* output: The output parameters of our model.\n",
    "  * `name`: Should correspond with the model output name. In this case, we're using the name automatically assigned by TensorFlow.\n",
    "  * `data_type`: Should correspond to the output's data type.\n",
    "  * `dims`: The dimensions of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat model_repository/wnd_tf/config.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Loading a Model\n",
    "\n",
    "Now, we can communicate with the Triton Inference Server and sent the request to load the model. We can verify this by using [curl](https://curl.haxx.se/) to make a `GET` request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -i triton:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's build a client to connect to our server. This [InferenceServerClient](https://github.com/triton-inference-server/client) object is what we'll be using to talk to Triton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonhttpclient\n",
    "\n",
    "import cudf\n",
    "import tritonclient.grpc as grpcclient\n",
    "import nvtabular.inference.triton as nvt_triton\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    triton_client = tritonhttpclient.InferenceServerClient(url=\"triton:8000\", verbose=True)\n",
    "    print(\"client created.\")\n",
    "except Exception as e:\n",
    "    print(\"channel creation failed: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that our server is ready to go by using [is_server_live](https://github.com/triton-inference-server/client/blob/12d8a2a7318ccb4a367a09a42b80feba53f3944a/src/python/library/tritonclient/grpc/__init__.py#L259). [get_model_repository_index](https://github.com/triton-inference-server/client/blob/12d8a2a7318ccb4a367a09a42b80feba53f3944a/src/python/library/tritonclient/grpc/__init__.py#L555) will also show what folders are in Triton's model repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_client.is_server_live()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_client.get_model_repository_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is configured, let's get the model loaded! First, we'll create a version for our model. By default, Triton loads the version in the first listed folder, so we'll use `1` for our version number.\n",
    "\n",
    "Finally, we'll copy our model into the server.\n",
    "\n",
    "We've set Triton's [Model Control Mode](https://github.com/triton-inference-server/server/blob/main/docs/model_management.md#model-control-mode-explicit) to `EXPLICIT`, meaning, it's not going to automatically pick up the model placed in it's directory. This is done on line 13 of our `docker-compose.yml` file in the [previous lab](3-02_docker.ipynb). We could have used [POLL](https://github.com/triton-inference-server/server/blob/main/docs/model_management.md) in order to do this, but it's not immediate when checking for changes.\n",
    "\n",
    "In order to load our model, we'll use [load_model](https://github.com/triton-inference-server/client/blob/12d8a2a7318ccb4a367a09a42b80feba53f3944a/src/python/library/tritonclient/grpc/__init__.py#L601). When needed, we can use [unload_model](https://github.com/triton-inference-server/client/blob/12d8a2a7318ccb4a367a09a42b80feba53f3944a/src/python/library/tritonclient/grpc/__init__.py#L634) when we want to remove it from the Triton server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"wnd_tf\"\n",
    "triton_client.load_model(model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is loaded, we can use [get_model_metadata](https://github.com/triton-inference-server/client/blob/12d8a2a7318ccb4a367a09a42b80feba53f3944a/src/python/library/tritonclient/grpc/__init__.py#L429) to see our model's inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_client.get_model_metadata(model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, time to shine! Let's make a request to our server!\n",
    "\n",
    "### 2. Sent requests for predictions\n",
    "\n",
    "We can use [InferInput](https://github.com/triton-inference-server/client/blob/12d8a2a7318ccb4a367a09a42b80feba53f3944a/src/python/library/tritonclient/grpc/__init__.py#L1449) to describe the tensors we'll be sending to the server. It needs the name of the input, the shape of the tensor we'll be passing to the server, and its datatype.\n",
    "\n",
    "Then, we can use [set_data_from_numpy](https://github.com/triton-inference-server/client/blob/12d8a2a7318ccb4a367a09a42b80feba53f3944a/src/python/library/tritonclient/grpc/__init__.py#L1513) to pass it a NumPy array.\n",
    "\n",
    "We'll use some fake data for now. The first row of our batch will have all `1`s and the second will have all `2`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "batch_size = 2\n",
    "inputs.append(tritonhttpclient.InferInput(\"user_index\", [batch_size, 1], \"INT64\"))\n",
    "inputs.append(tritonhttpclient.InferInput(\"item_index\", [batch_size, 1], \"INT64\"))\n",
    "inputs.append(tritonhttpclient.InferInput(\"brand_index\", [batch_size, 1], \"INT64\"))\n",
    "inputs.append(tritonhttpclient.InferInput(\"price_filled\", [batch_size, 1], \"FP32\"))\n",
    "inputs.append(tritonhttpclient.InferInput(\"salesRank_Electronics\", [batch_size, 1], \"FP32\"))\n",
    "inputs.append(tritonhttpclient.InferInput(\"category_0_2_index\", [batch_size, 1], \"INT32\"))\n",
    "inputs.append(tritonhttpclient.InferInput(\"category_1_2_index\", [batch_size, 1], \"INT32\"))\n",
    "\n",
    "inputs[0].set_data_from_numpy(np.array([[1], [2]], dtype=np.int64))\n",
    "inputs[1].set_data_from_numpy(np.array([[1], [2]], dtype=np.int64))\n",
    "inputs[2].set_data_from_numpy(np.array([[1], [2]], dtype=np.int64))\n",
    "inputs[3].set_data_from_numpy(np.array([[1.0], [2.0]], dtype=np.float32))\n",
    "inputs[4].set_data_from_numpy(np.array([[1.0], [2.0]], dtype=np.float32))\n",
    "inputs[5].set_data_from_numpy(np.array([[1], [2]], dtype=np.int32))\n",
    "inputs[6].set_data_from_numpy(np.array([[1], [2]], dtype=np.int32))\n",
    "\n",
    "outputs.append(\n",
    "    tritonhttpclient.InferRequestedOutput(\"tf.__operators__.add\", binary_data=False)\n",
    ")\n",
    "results = triton_client.infer(model_name, inputs, outputs=outputs).get_response()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll get a bunch of data returned from our response, but the important one is the `\"data\"` at the very end. That's our prediction from our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"outputs\"][0][\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a lot of work for only two predictions. Can we give it something meatier? We have loaded in the data from our previous labs. Let's try running our validation data from lab2 through the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"./data/task_2_wide_and_deep.csv\")\n",
    "ratings = ratings[ratings[\"valid\"]]\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to be a little more efficient with our code this time. We'll use a `for` loop to construct our inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    ('user_index', \"INT64\"),\n",
    "    ('item_index', \"INT64\"),\n",
    "    ('brand_index', \"INT64\"),\n",
    "    ('price_filled', \"FP32\"),\n",
    "    ('salesRank_Electronics', \"FP32\"),\n",
    "    ('category_0_2_index', \"INT32\"),\n",
    "    ('category_1_2_index', \"INT32\")\n",
    "]\n",
    "\n",
    "dtypes = {\n",
    "    \"INT32\": np.int32,\n",
    "    \"INT64\": np.int64,\n",
    "    \"FP32\": np.float32\n",
    "}\n",
    "\n",
    "inputs = []\n",
    "batch_size = 64\n",
    "for column in columns:\n",
    "    name = column[0]\n",
    "    dtype = dtypes[column[1]]\n",
    "    data = np.expand_dims(np.array(ratings.head(batch_size)[name], dtype=dtype), axis=-1)\n",
    "    inputs.append(tritonhttpclient.InferInput(name, [batch_size, 1], column[1]))\n",
    "    inputs[-1].set_data_from_numpy(data)\n",
    "\n",
    "results = triton_client.infer(model_name, inputs, outputs=outputs).get_response()\n",
    "\n",
    "print(\"\\nprediction results:\\n\", results[\"outputs\"][0][\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Server Metrics\n",
    "\n",
    "Not only can we scale serving our data, but we can also gather metrics on our model as well. This is crucial, finding the right metric to optimize for with recommender systems is not a trivial task. Check out this [great paper](https://www.kdd.org/exploration_files/19-1-Article3.pdf) explaining common pitfalls.\n",
    "\n",
    "The short version is this:\n",
    "* Recommender systems create a feedback loop between users and recommendations. Popular items train our models that these are good recommendations, thus serving them to more users and perpetuating the loop.\n",
    "* Try to avoid metrics that are biased by human behavior. For instance, click through rate is one commonly used in the advertisement space, but if not careful, using this will train the model which position on a web page is popular as opposed to the content.\n",
    "\n",
    "At the end of the day, the goal is to increase user engagement. Triton automatically serves usage metrics using [Prometheus](https://prometheus.io/). Copy and paste the URL (web address) for this notebook and set it to `my_url` below. Run the cell to see the metrics for our model. [Here](https://github.com/triton-inference-server/server/blob/main/docs/metrics.md) is a list of available metrics, but a good one to start with is `nv_inference_count` which displays how many predictions have been made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "my_url = \"COPY_NOTEBOOK_URL\"\n",
    "prometheus_url = my_url.rsplit(\".com\", 1)[0] + \".com:9090/graph\"\n",
    "IPython.display.IFrame(prometheus_url, width=700, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap Up\n",
    "\n",
    "We can take this a little further and hook these results into a service like [Grafana](https://grafana.com/) as explained in [this excellent blog post](https://blog.einstein.ai/benchmarking-tensorrt-inference-server/) by the SalesForce team, but for now, we have all the pieces to build an end-to-end recommender system.\n",
    "\n",
    "Feeling ready? Head on over to [the next lab](3-04_assessment.ipynb) to put these new skills into action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./images/DLI_Header.png\"></a></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
