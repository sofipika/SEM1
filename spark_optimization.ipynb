{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization scrypt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "w8WhR7KY7CZR"
   },
   "outputs": [],
   "source": [
    "param_grid = { 'spark.serializer'                : ['org.apache.spark.serializer.KryoSerializer', 'org.apache.spark.serializer.JavaSerializer']    # выбрать одно значение для всех случаев\n",
    "              ,'spark.kryoserializer.buffer.max' : [32, 64, 128, 256]      # выбрать одно значение для всех случаев\n",
    "              ,'spark.kryoserializer.buffer'     : [32, 64, 128, 256]      # выбрать одно значение для всех случаев\n",
    "              ,'spark.executor.cores'            : [2, 3, 4, 5]            # выбрать одно значение для всех случаев\n",
    "              ,'spark.network.timeout'           : [120, 500, 1000, 3000]  # выбрать одно значение для всех случаев\n",
    "              ,'spark.driver.memory'             : [2, 4, 8, 16]\n",
    "              ,'spark.executor.memory'           : [8, 16, 24, 32]\n",
    "              ,'spark.executor.memoryOverhead'   : [1, 2, 4, 8]\n",
    "              ,'spark.memory.fraction'           : [0.6, 0.7, 0.8, 0.9]\n",
    "              ,'spark.memory.storageFraction'    : [0.5, 0.6, 0.7, 0.8]\n",
    "              ,'spark.shuffle.file.buffer'       : [64, 128, 256, 512]\n",
    "              #,'spark.shuffle.manager'           : ['sort']\n",
    "              #,'spark.shuffle.compress'          : [True]\n",
    "              #,'spark.shuffle.spill.compress'    : [True]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "yQ1pMyhQ7-Sp",
    "outputId": "cc276c42-427c-4ad0-b30b-f9458792cac3"
   },
   "outputs": [],
   "source": [
    "def iterator_dict(**kwargs):\n",
    "    current_key = next(iter(kwargs))\n",
    "    if len(kwargs) == 1:\n",
    "        for item in kwargs[current_key]:\n",
    "            yield {current_key: item}\n",
    "    else:\n",
    "        kwargs_copy = {\n",
    "            key: val\n",
    "            for key, val in kwargs.items()\n",
    "            if key != current_key\n",
    "        }\n",
    "        for item in kwargs[current_key]:\n",
    "            for sub_dict in iterator_dict(**kwargs_copy):\n",
    "                out_dict = {current_key: item}\n",
    "                out_dict.update(sub_dict)\n",
    "                yield out_dict\n",
    "\n",
    "# для расчета количества итераций\n",
    "#i = 0\n",
    "#param_grid_iter = iterator_dict(**param_grid)\n",
    "#for item in param_grid_iter:\n",
    "#  i+=1\n",
    "#print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "id": "pPwArM-sZ3SW"
   },
   "outputs": [],
   "source": [
    "def start_spark_session(        \n",
    "                        application_name='gsa___bla bla bla pyu',\n",
    "                        type_serializer='org.apache.spark.serializer.JavaSerializer',\n",
    "                        kryo_buffer_max=32,   kryo_buffer=32,              executor_cores=1,\n",
    "                        driver_memory=1,      executor_memory=1,           memory_overhead=1,\n",
    "                        memory_fraction=0.6,  memory_storageFraction=0.5,  shuffle_file_buffer=128,\n",
    "                        network_timeout=120):\n",
    "    conf = SparkConf()\n",
    "    conf.setAll(\n",
    "       [\n",
    "            ('spark.serializer', type_serializer),\n",
    "            ('spark.kryoserializer.buffer.max', '{}m'.format(kryo_buffer_max)),\n",
    "            ('spark.kryoserializer.buffer', '{}m'.format(kryo_buffer)),\n",
    "            ('spark.memory.fraction', memory_fraction),\n",
    "            ('spark.memory.storageFraction', memory_storageFraction),\n",
    "            ('spark.shuffle.file.buffer', shuffle_file_buffer),\n",
    "            ('spark.executor.cores', executor_cores),\n",
    "            ('spark.driver.memory', '{}g'.format(driver_memory)),\n",
    "            ('spark.executor.memory', '{}g'.format(executor_memory)),\n",
    "            ('spark.yarn.executor.memoryOverhead', '{}g'.format(memory_overhead)),\n",
    "            ('spark.network.timeout','{}s'.format(network_timeout)),\n",
    "            ('spark.yarn.queue','team_mmb_prom_reserv'),\n",
    "            ('spark.hive.mapred.supports.subdirectories', 'true'),\n",
    "            ('spark.sql.hive.manageFilesourcePartitions', 'true'),\n",
    "            ('spark.hadoop.mapreduce.input.fileinputformat.input.dir.recursive', 'true'),\n",
    "            ('spark.sql.catalogImplementation', 'hive'),\n",
    "            ('spark.hadoop.metastore.skip.load.functions.on.init', 'true'),\n",
    "            ('spark.sql.optimizer.metadataOnly', 'true'),\n",
    "            ('hive.exec.dynamic.partition', 'true'),\n",
    "            ('hive.exec.dynamic.partition.mode', 'nonstrict'),\n",
    "            ('spark.shuffle.manager', 'sort'),\n",
    "            ('spark.shuffle.compress', 'true'),\n",
    "            ('spark.shuffle.spill.compress', 'true'),\n",
    "            ('spark.shuffle.service.enabled', 'true'),\n",
    "            ('spark.sql.broadcastTimeout', '3000'),\n",
    "            ('spark.driver.maxResultSize', '10g'),\n",
    "            ('spark.hadoop.validateOutputSpecs', 'false'),\n",
    "            ('spark.sql.codegen.wholeStage', 'true'),\n",
    "            ('spark.sql.hive.convertMetastoreParquet', 'false'),\n",
    "            ('spark.sql.parquet.readLegacyFormat', 'true'),\n",
    "            ('spark.sql.parquet.binaryAsString', 'true'),\n",
    "            ('spark.sql.sources.partitionColumnTypeInference.enabled', 'true'),\n",
    "            ('spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation','true'),\n",
    "            ('spark.dynamicAllocation.enabled', 'true'),\n",
    "            ('spark.dynamicAllocation.executorIdleTimeout', '120s'),\n",
    "            ('spark.dynamicAllocation.cachedExecutorIdleTimeout', '600s'),\n",
    "            ('spark.dynamicAllocation.shuffleTracking.enabled', 'true'),\n",
    "            ('spark.dynamicAllocation.initialExecutors', '0'),\n",
    "            ('spark.dynamicAllocation.minExecutors', '0'),\n",
    "            ('spark.dynamicAllocation.maxExecutors', '300'), #уточнить на примера ПРОМа какой максимуум? 300 или 500?)\n",
    "            ('spark.yarn.access.hadoopFileSystems', 'hdfs://hdfsgw,hdfs://arnsdpldbr2,hdfs://arnsdpsbx,hdfs://arnsdpsmd2,hdfs://spsdpsmd,hdfs://arnsdprisk')\n",
    "    ])\n",
    "    \n",
    "    spark = SparkSession.builder.master('yarn-client').appName(application_name).config(conf=conf).getOrCreate()\n",
    "    print('Context ready: {}'.format(spark))\n",
    "    #spark.sql('use {}'.format(default_schema));\n",
    "    #print('Set default schema: {}'.format(default_schema))\n",
    "    return spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logger = logging.getLogger(\"spark_logsss\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter(\n",
    "    fmt=\"[{levelname[0]} {asctime}] {message}\",\n",
    "    datefmt=\"%y%m%d %H:%M:%S\",\n",
    "    style=\"{\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#logging.getLogger(\"spark\").handlers.clear()\n",
    "#logger.handlers.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not logger.handlers:\n",
    "    #console_handler = logging.StreamHandler(stream=sys.stdout)\n",
    "    file_handler = logging.FileHandler(\"logs_params.txt\")\n",
    "\n",
    "    #console_handler.setFormatter(formatter)\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    #logger.addHandler(console_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "#logger.handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<FileHandler /home/18289972_omega-sbrf-ru/notebooks/logs_params.txt (NOTSET)>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijExuYfpwhcv"
   },
   "source": [
    "## Исследуем 'spark.serializer' и размер буферов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "id": "07DINhgY7C3s",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79c4133a90>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79c413d278>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f792c89b9e8>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79c413d7f0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79c4133470>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f792c89bcc0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade95b38>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79c405efd0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade9b860>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79c412d080>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade9b128>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade9b5c0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79c4133898>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade95c18>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79c412de10>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79c4133c88>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79c413dac8>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade365c0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade8d978>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79c4133e48>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade957f0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79adea10f0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade369b0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79cc4e4b38>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f792c89a4e0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade95048>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79c412d518>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79adea1940>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade36198>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade480b8>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade48ac8>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade59198>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79c412dd68>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79adea94e0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade4e438>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade40ef0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79adea90f0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79adea9c88>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade36630>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade8d160>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade61898>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79adea1fd0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addee048>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade9bbe0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79c412d5f8>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade6a6d8>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79c412d240>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade46630>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade46908>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade61198>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade619b0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79adea9358>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade40588>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade9bcc0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade00278>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade06278>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade9ba58>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addfc9b0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addfc518>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f792c89b8d0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addee278>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade46908>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade6ab70>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade06710>\n"
     ]
    }
   ],
   "source": [
    "logger.info('\\n'*1)\n",
    "logger.info('spark.serializer')\n",
    "logger.info('\\n'*1)\n",
    "param_grid_serializer = { 'spark.serializer'                : ['org.apache.spark.serializer.KryoSerializer', 'org.apache.spark.serializer.JavaSerializer']\n",
    "                         ,'spark.kryoserializer.buffer.max' : [32, 64, 128, 256]\n",
    "                         ,'spark.kryoserializer.buffer'     : [32, 64, 128, 256]\n",
    "                        }\n",
    "#param_grid_iter_srlzr = iterator_dict(**param_grid_serializer)\n",
    "start_calc_fnc = [start_calc_3\n",
    "                  ,start_calc_2]\n",
    "min_time = 100000000000\n",
    "\n",
    "for func in start_calc_fnc:\n",
    "    logger.info(func.__name__)\n",
    "    param_grid_iter_srlzr = iterator_dict(**param_grid_serializer)\n",
    "    min_time = 100000000000\n",
    "    for item in param_grid_iter_srlzr:\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "        spark = start_spark_session(application_name = 'pi pu pa',                   type_serializer=item['spark.serializer'],\n",
    "                            kryo_buffer_max=item['spark.kryoserializer.buffer.max'], kryo_buffer=item['spark.kryoserializer.buffer'])\n",
    "        start_time = time.time()\n",
    "        # добавить функцию для выполнения кода\n",
    "        func()\n",
    "        if (time.time() - start_time) <= min_time:\n",
    "            min_time = (time.time() - start_time)\n",
    "            logger.info('##########################################')\n",
    "            logger.info(f'Оптимальные параметры! {min_time}')\n",
    "            logger.info(\"Характеристики: type_serializer={}, kryo_buffer_max={}, kryo_buffer={}, остальные параметры дефолтные\"\n",
    "                               .format(item['spark.serializer'], item['spark.kryoserializer.buffer.max'],item['spark.kryoserializer.buffer']))\n",
    "            logger.info('##########################################')\n",
    "        else:\n",
    "            logger.info(f'Неоптимальные параметры! {min_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1N0gPb88NMl"
   },
   "source": [
    "## Исследуем 'spark.executor.cores'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "YOler92n7Hhb",
    "outputId": "ea3af992-c169-47fa-eca4-8ba79f4b9915",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f3613e59f28>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f3615954198>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f36b58d5630>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f36995a2278>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f36bafcf6a0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f3699816b00>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f3699585eb8>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f36995a2710>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f36b58d58d0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f36998169e8>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f3699816828>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f36998013c8>\n"
     ]
    }
   ],
   "source": [
    "logger.info('\\n'*1)\n",
    "logger.info('spark.executor.cores')\n",
    "logger.info('\\n'*1)\n",
    "start_calc_fnc = [start_calc_1\n",
    "                  ,start_calc_2\n",
    "                  ,start_calc_3]\n",
    "min_time = 100000000000\n",
    "\n",
    "for func in start_calc_fnc:\n",
    "    logger.info(func.__name__)\n",
    "    min_time = 100000000000\n",
    "    for item in param_grid['spark.executor.cores']:\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "        spark = start_spark_session(application_name = 'pi pu pa', executor_cores=item)\n",
    "        start_time = time.time()\n",
    "        # добавить функцию для выполнения кода\n",
    "        func()\n",
    "        if (time.time() - start_time) <= min_time:\n",
    "            min_time = (time.time() - start_time)\n",
    "            logger.info('##########################################')\n",
    "            logger.info(f'Оптимальные параметры! {min_time}')\n",
    "            logger.info(\"Характеристики: executor_cores={}, остальные параметры дефолтные\".format(item))\n",
    "            logger.info('##########################################')\n",
    "            \n",
    "        else:\n",
    "            logger.info(f'Неоптимальные параметры! {min_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDSNp6jO8Zym"
   },
   "source": [
    "## Исследуем 'spark.network.timeout'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "xapqOoRC8GcM",
    "outputId": "1879b3c9-50a0-446d-b2c8-6095a03b5437",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addada58>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade1fda0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addc1da0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addb5898>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addce9e8>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79adda63c8>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addb5dd8>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addb5358>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addbf278>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79adddb5f8>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79adda6fd0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade1f940>\n"
     ]
    }
   ],
   "source": [
    "logger.info('\\n'*1)\n",
    "logger.info('spark.network.timeout')\n",
    "logger.info('\\n'*1)\n",
    "start_calc_fnc = [start_calc_1\n",
    "                  ,start_calc_2\n",
    "                  ,start_calc_3]\n",
    "min_time = 100000000000\n",
    "\n",
    "for func in start_calc_fnc:\n",
    "    logger.info(func.__name__)\n",
    "    min_time = 100000000000\n",
    "    for item in param_grid['spark.network.timeout']:\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "        spark = start_spark_session(application_name = 'pi pu pa', network_timeout=item)\n",
    "        start_time = time.time()\n",
    "        # добавить функцию для выполнения кода\n",
    "        func()\n",
    "        if (time.time() - start_time) <= min_time:\n",
    "            min_time = (time.time() - start_time)\n",
    "            logger.info('##########################################')\n",
    "            logger.info(f'Оптимальные параметры! {min_time}')\n",
    "            logger.info(\"Характеристики: network_timeout={}, остальные параметры дефолтные\".format(item))\n",
    "            logger.info('##########################################')\n",
    "            \n",
    "        else:\n",
    "            logger.info(f'Неоптимальные параметры! {min_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfoQuTcuR6pA"
   },
   "source": [
    "## Исследуем 'spark.shuffle.file.buffer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "pfDObvOp8Ge6",
    "outputId": "aa4edffe-096a-46cf-85a6-8ca64c18c02c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79adddbbe0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79adda6a20>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79adda6c18>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade6a518>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addfceb8>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addc1048>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade1f7b8>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addf3400>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addfc9b0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addbff28>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addadef0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addcea58>\n"
     ]
    }
   ],
   "source": [
    "logger.info('\\n'*1)\n",
    "logger.info('spark.shuffle.file.buffer')\n",
    "logger.info('\\n'*1)\n",
    "start_calc_fnc = [start_calc_1\n",
    "                  ,start_calc_2\n",
    "                  ,start_calc_3]\n",
    "min_time = 100000000000\n",
    "\n",
    "\n",
    "for func in start_calc_fnc:\n",
    "    logger.info(func.__name__)\n",
    "    min_time = 100000000000\n",
    "    for item in param_grid['spark.shuffle.file.buffer']:\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "        spark = start_spark_session(application_name = 'pi pu pa', shuffle_file_buffer=item)\n",
    "        start_time = time.time()\n",
    "        # добавить функцию для выполнения кода\n",
    "        func()\n",
    "        if (time.time() - start_time) <= min_time:\n",
    "            min_time = (time.time() - start_time)\n",
    "            logger.info('##########################################')\n",
    "            logger.info(f'Оптимальные параметры! {min_time}')\n",
    "            logger.info(\"Характеристики: shuffle_file_buffer={}, остальные параметры дефолтные\".format(item))\n",
    "            logger.info('##########################################')\n",
    "            \n",
    "        else:\n",
    "            logger.info(f'Неоптимальные параметры! {min_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3g59JxeR86i"
   },
   "source": [
    "## Исследуем 'spark.memory.fraction' и 'spark.memory.storageFraction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ZPzDkw388Gk6",
    "outputId": "9b1fb251-2ae6-4170-a4cc-cb143d00cae7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addc1d68>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addee940>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade1ff98>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade2e9b0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade61c88>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade2ef98>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addc1f60>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addc1828>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f792c89ae10>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addc1320>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addb5048>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade1f9b0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79adddb160>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade2ed68>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade9bfd0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade9b6d8>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79c403a6a0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addf3048>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f792c89afd0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addee470>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade9b4a8>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade4e550>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addee470>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79cc4ee9b0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade4e128>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f792c89a6d8>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade8d080>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade4eda0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f792c89b588>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ce3fa3c8>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade46c50>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade1f438>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade8dc50>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addf36d8>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade9b198>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addc1400>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addd35f8>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addd3160>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade8dd30>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade4e5f8>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79addd3c50>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade4eeb8>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79c4133d68>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79c41330b8>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade9e908>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade462b0>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade46320>\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f79ade2ea90>\n"
     ]
    }
   ],
   "source": [
    "logger.info('\\n'*1)\n",
    "logger.info( 'spark.memory.fraction and spark.memory.storageFraction')\n",
    "logger.info('\\n'*1)\n",
    "start_calc_fnc = [start_calc_1\n",
    "                  ,start_calc_2\n",
    "                  ,start_calc_3]\n",
    "min_time = 100000000000\n",
    "\n",
    "param_grid_fraction = { 'spark.memory.fraction'           : [0.6, 0.7, 0.8, 0.9]\n",
    "                       ,'spark.memory.storageFraction'    : [0.5, 0.6, 0.7, 0.8]\n",
    "                      }\n",
    "param_grid_iter_fraction = iterator_dict(**param_grid_fraction)\n",
    "\n",
    "for func in start_calc_fnc:\n",
    "    logger.info(func.__name__)\n",
    "    min_time = 100000000000\n",
    "    param_grid_iter_fraction = iterator_dict(**param_grid_fraction)\n",
    "    for item in param_grid_iter_fraction:\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "        spark = start_spark_session(application_name = 'pi pu pa', memory_fraction=item['spark.memory.fraction'], memory_storageFraction=item['spark.memory.storageFraction'])\n",
    "        start_time = time.time()\n",
    "        # добавить функцию для выполнения кода\n",
    "        func()\n",
    "        if (time.time() - start_time) <= min_time:\n",
    "            min_time = (time.time() - start_time)\n",
    "            logger.info('##########################################')\n",
    "            logger.info(f'Оптимальные параметры! {min_time}')\n",
    "            logger.info(\"Характеристики: memory_fraction={}, memory_storageFraction={}, остальные параметры дефолтные\".format(item['spark.memory.fraction'], item['spark.memory.storageFraction']))\n",
    "            logger.info('##########################################')\n",
    "            \n",
    "        else:\n",
    "            logger.info(f'Неоптимальные параметры! {min_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции для исследования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def start_calc_1():\n",
    "    spark.sql(\"\"\"\"\"\").limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def start_calc_2():\n",
    "    spark.sql(\"\"\"\"\"\").limit(2) #.write.format('parquet').mode('overwrite').saveAsTable('spark_optim_trans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "id": "2Sq0x3-SSBoq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def start_calc_3():\n",
    "    spark.sql(\"\"\"\n",
    "\"\"\").limit(2) #.write.format('parquet').mode('overwrite').saveAsTable('spark_optim_fns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "### Снять поток (wf_id) с расписания\n",
    "def delete_sched(CTL_URL, wf_id):\n",
    "    try:\n",
    "        print(\"delete sched (wf_id: {})\".format(wf_id))\n",
    "        headers = {'Content-Type':'application/json'}\n",
    "        wf_id = int(wf_id)\n",
    "        response = requests.delete(\"{0}/v1/api/wf/sched/{1}\".format(CTL_URL,wf_id), headers = headers)\n",
    "        print(response.status_code)\n",
    "    except:\n",
    "        print(\"ERROR: Unable to delete sched!\")\n",
    "        raise\n",
    "\n",
    "### Вернуть поток(wf_id) на расписание\n",
    "def put_sched(CTL_URL, wf_id):\n",
    "    try:\n",
    "        print(\"put sched (wf_id: {})\".format(wf_id))\n",
    "        headers = {'Content-Type':'application/json'}\n",
    "        wf_id = int(wf_id)\n",
    "        response = requests.put(\"{0}/v1/api/wf/sched/{1}\".format(CTL_URL,wf_id), headers = headers)\n",
    "        print(response.status_code)\n",
    "    except:\n",
    "        print(\"ERROR: Unable to delete sched!\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Расчет с помощью запуска потоков (автоматическое тестирование)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "LOAD_DC_CDM_RISK_MMB_SCORE_RBL_HIST_ON CUSTOM_PARAM_DC_CDM_RISK_MMB_SCORE_RBL_HIST\n",
      "LOAD_DC_COMPACTOR_ON CUSTOM_PARAM_DC_COMPACTOR\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# Модуль обучения (передаем номер потока, который начал дольше считаться) CHECK_SPARK_PARAMS (SPARMS):\n",
    "# запустить каждый модуль потока отдельно, вырубив остальные модули и чекаться на этом\n",
    "\n",
    "# 1) поискать в потоке все модули по параметрам потока, ориентируясь на custom_param\n",
    "# 2) в цикле пройтись и запустить поток с одним включенным модулем и чекать время выполнения\n",
    "# 3) сохранить результаты в таблице:\n",
    "#       Номер потока; название моделя; текущая дата расчета; параметры\n",
    "import requests\n",
    "import json\n",
    "from re import search\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "wf_id = 11111\n",
    "CTL_URL = ''\n",
    "df = spark.table('spark_params')\n",
    "\n",
    "# получить параметры потока\n",
    "def get_param_wf_id(CTL_URL, wf_id):\n",
    "    try:\n",
    "        headers = {'Content-Type':'application/json'}\n",
    "        response = requests.get(\"{0}/v1/api/params/wf/{1}\".format(CTL_URL, int(wf_id)), headers = headers)\n",
    "        print(response.status_code)\n",
    "        json_dt = response.json()\n",
    "    except:\n",
    "        print(\"ERROR: wf_id does not exist\")\n",
    "        raise\n",
    "    return json_dt\n",
    "\n",
    "# запустить поток \n",
    "def run_wf_with_param(CTL_URL, wf_id, load_param):\n",
    "    try:\n",
    "        headers = {'Content-Type':'application/json'}\n",
    "        #data_param = json.dumps(load_param)\n",
    "        #data_param_cr = re.sub(\"\\\"\",\"\\\\\\\"\",data_param)\n",
    "        #data_param_cree = \"\".join(data_param_cr.split())\n",
    "        response = requests.post(\"{0}/v1/api/wf/sched/{1}\".format(CTL_URL, wf_id), headers = headers, json = load_param)\n",
    "        print(response.status_code)\n",
    "        json_dt = response.json()\n",
    "    except:\n",
    "        print(\"ERROR: Unable to run wf!\")\n",
    "        raise\n",
    "    return json_dt\n",
    "\n",
    "# получить статус загрузки\n",
    "def get_status_loading_id(CTL_URL, loading_id):\n",
    "    try:\n",
    "        headers = {'Content-Type':'application/json'}\n",
    "        response = requests.get(\"{0}/v1/api/loading/{1}\".format(CTL_URL, loading_id), headers = headers)\n",
    "        print(response.status_code)\n",
    "        json_dt = response.json()\n",
    "    except:\n",
    "        print(\"ERROR: Unable to loading!\")\n",
    "        raise\n",
    "    return json_dt\n",
    "\n",
    "\n",
    "# завершить загрузку\n",
    "def complete_loading(CTL_URL, loading_id):\n",
    "    try:\n",
    "        print(\"delete loading (ctl_loading: {})\".format(loading_id))\n",
    "        headers = {'Content-Type':'application/json'}\n",
    "        response = requests.delete(\"{0}/v1/api/loading/{1}\".format(CTL_URL, int(loading_id)), headers = headers)\n",
    "        print(response.status_code)\n",
    "    except:\n",
    "        print(\"ERROR: Unable to delete loading!\")\n",
    "        raise\n",
    "\n",
    "\n",
    "custom_param = {'CUSTOM_PARAM':'N/A'}   # dict параметров custom_param_dc у потока\n",
    "load_param = {}                         # dict параметров load_dc у потока\n",
    "\n",
    "df_json = get_param_wf_id(CTL_URL, wf_id)\n",
    "for i in df_json:\n",
    "    if search('CUSTOM_PARAM',i['param']):           # добавить флаг джава или питон трансформации\n",
    "        custom_param[i['param']]=i['prior_value']\n",
    "        #print(i['param'],i['prior_value'])ы\n",
    "    elif search('LOAD',i['param']) and i['prior_value'] != 'N' and i['param'] not in ('LOAD_TRANSFER_FROM_SNP_TO_HIST_ON','LOAD_DC_DM_CLEAN_ON'):\n",
    "        load_param[i['param']]=i['prior_value']\n",
    "        #print(i['param'],i['prior_value'])\n",
    "\n",
    "moduls_cl_param = {} # dict соответствия включенных модулей и параметров\n",
    "for c_key in custom_param:\n",
    "    for l_key in load_param:\n",
    "        if str(c_key)[13:] == str(l_key)[5:-3]:\n",
    "            print(l_key, c_key)\n",
    "            moduls_cl_param[l_key] = c_key   # соответствие включенных модулей и параметров\n",
    "\n",
    "# добавить запуск потока с включенным одним модулем, чтобы чекать время для него в общий алгос\n",
    "# затем изменить параметр custom_param_dc у потока после нахождения минимума!!!\n",
    "param_grid = {# 'spark.serializer'                : ['org.apache.spark.serializer.KryoSerializer', 'org.apache.spark.serializer.JavaSerializer']    # выбрать одно значение для всех случаев\n",
    "              #,'spark.kryoserializer.buffer.max' : [32, 64, 128, 256]      # выбрать одно значение для всех случаев\n",
    "              #,'spark.kryoserializer.buffer'     : [32, 64, 128, 256]      # выбрать одно значение для всех случаев\n",
    "               'spark.executor.cores'            : [4, 5]            # выбрать одно значение для всех случаев\n",
    "              #,'spark.network.timeout'           : [120, 500, 1000, 3000]  # выбрать одно значение для всех случаев\n",
    "              ,'spark.network.timeout'           : [1000]  # выбрать одно значение для всех случаев\n",
    "              ,'spark.driver.memory'             : [2, 4, 8, 16]\n",
    "              ,'spark.executor.memory'           : [8, 16, 24, 32]\n",
    "              #,'spark.executor.memoryOverhead'   : [1, 2, 4, 8]\n",
    "              #,'spark.memory.fraction'           : [0.6, 0.7, 0.8, 0.9]\n",
    "              #,'spark.memory.storageFraction'    : [0.5, 0.6, 0.7, 0.8]\n",
    "              #,'spark.shuffle.file.buffer'       : [64, 128, 256, 512]\n",
    "              #,'spark.shuffle.manager'           : ['sort']\n",
    "              #,'spark.shuffle.compress'          : [True]\n",
    "              #,'spark.shuffle.spill.compress'    : [True]\n",
    "              }\n",
    "\n",
    "param_grid_iter = iterator_dict(**param_grid)\n",
    "# самое минимальное и оптимальное время выполнения\n",
    "min_time = float(100000000.0)\n",
    "# самое минимальные и оптимальные параметры для выполнения\n",
    "#min_params = {}\n",
    "new_params = []\n",
    "for key_load in load_param:\n",
    "    load_param_copy = load_param.copy()\n",
    "    for key_copy in load_param_copy:\n",
    "        load_param_copy.update({key_copy: 'N'})\n",
    "    param_grid_iter = iterator_dict(**param_grid)\n",
    "    min_time = float(100000000.0)\n",
    "    for item in param_grid_iter:\n",
    "        #if spark:\n",
    "        #    spark.stop()\n",
    "        #spark = start_spark_session( #type_serializer=item['spark.serializer'],\n",
    "        #                    #kryo_buffer_max=item['spark.kryoserializer.buffer.max'], kryo_buffer  =  item['spark.kryoserializer.buffer'], \n",
    "        #                    executor_cores =item['spark.executor.cores'],            driver_memory = item['spark.driver.memory'], \n",
    "        #                    executor_memory=item['spark.executor.memory'],           memory_overhead=item['spark.executor.memoryOverhead'],\n",
    "        #                    #memory_fraction=item['spark.memory.fraction'],           memory_storageFraction=item['spark.memory.storageFraction'], \n",
    "        #                    #shuffle_file_buffer=item['spark.shuffle.file.buffer'],   \n",
    "        #                    network_timeout=item['spark.network.timeout'])\n",
    "        #print(spark)\n",
    "        load_param_copy[key_load] = 'Y'\n",
    "\n",
    "        #print(load_param_copy)\n",
    "        custom_param_for_this_load = moduls_cl_param[key_load]\n",
    "        #print(custom_param_for_this_load)\n",
    "        custom_param_key = custom_param[custom_param_for_this_load]\n",
    "        logger.info(custom_param_key)\n",
    "        custom_param_key_list = [\"\".join(x.rstrip()) for x in custom_param_key.split('--') if x != '']\n",
    "        #print(custom_param_key_list)\n",
    "        params = {'spark.serializer'                                 :'org.apache.spark.serializer.KryoSerializer',\n",
    "                  'spark.kryoserializer.buffer.max'                  :'32m',\n",
    "                  'spark.kryoserializer.buffer'                      :'32m',\n",
    "                  'spark.memory.fraction'                            :'0.6',\n",
    "                  'spark.memory.storageFraction'                     :'0.5',\n",
    "                  'spark.shuffle.file.buffer'                        :'128',\n",
    "                  'spark.shuffle.manager'                            :'sort',\n",
    "                  'spark.shuffle.compress'                           :'true',\n",
    "                  'spark.shuffle.spill.compress'                     :'true',\n",
    "                  'spark.shuffle.service.enabled'                    :'true',\n",
    "                  'spark.dynamicAllocation.enabled'                  :'true',\n",
    "                  'spark.dynamicAllocation.executorIdleTimeout'      :'120s',\n",
    "                  'spark.dynamicAllocation.cachedExecutorIdleTimeout':'600s',\n",
    "                  'spark.dynamicAllocation.initialExecutors'         :'0',\n",
    "                  'spark.dynamicAllocation.minExecutors'             :'0',\n",
    "                  'spark.dynamicAllocation.maxExecutors'             :'300'\n",
    "                }\n",
    "        \n",
    "        for i in custom_param_key_list:\n",
    "            if 'num-executors' in i or 'executor-memory' in i or 'executor-cores' in i or 'driver-memory' in i:\n",
    "                key, value = i.split(\" \")\n",
    "                if key == 'num-executors':\n",
    "                    continue\n",
    "                elif key == 'executor-memory':\n",
    "                    params.update({'spark.executor.memory': value})\n",
    "                elif key == 'executor-cores':\n",
    "                    params.update({'spark.executor.cores': value})\n",
    "                elif key == 'driver-memory':\n",
    "                    params.update({'spark.driver.memory': value})\n",
    "            else:\n",
    "                key, value = i.replace('conf ', '').split(\"=\")\n",
    "                params.update({key: value})\n",
    "        params.update(item)\n",
    "        params.update({'spark.executor.memoryOverhead': round(params.get('spark.executor.memory')*1024*0.1)})  # 1024*0.1 (переводим гегабайты в мегабайты а затем берем 10%)\n",
    "        \n",
    "        custom_param_final_str = \"--conf spark.serializer={} --conf spark.kryoserializer.buffer.max={} --conf spark.kryoserializer.buffer={} --conf spark.executor.cores={} --conf spark.driver.memory={}g \\\n",
    "--conf spark.executor.memory={}g --conf spark.executor.memoryOverhead={}m --conf spark.memory.fraction={} --conf spark.memory.storageFraction={} --conf spark.shuffle.file.buffer={} \\\n",
    "--conf spark.network.timeout={}s\".format(params['spark.serializer'],              params['spark.kryoserializer.buffer.max'], params['spark.kryoserializer.buffer'], \n",
    "                                         params['spark.executor.cores'],          params['spark.driver.memory'],             params['spark.executor.memory'], \n",
    "                                         params['spark.executor.memoryOverhead'], params['spark.memory.fraction'],           params['spark.memory.storageFraction'], \n",
    "                                         params['spark.shuffle.file.buffer'],     params['spark.network.timeout'])\n",
    "        \n",
    "        params_all_for_wf = load_param_copy.copy()\n",
    "        params_all_for_wf.update({custom_param_for_this_load: custom_param_final_str})\n",
    "        logger.info(params_all_for_wf)\n",
    "        # break\n",
    "        \n",
    "        # брать время с параметров загрузки по курлу\n",
    "        # start_time = time.time()\n",
    "        \n",
    "        loading_id = run_wf_with_param(CTL_URL, wf_id, params_all_for_wf)[1]      # запуск потока с включенным одним модулем\n",
    "        status_load = get_status_loading_id(CTL_URL, loading_id)\n",
    "        while (status_load['alive']=='ACTIVE' and (status_load['status'] in (\"INIT\",\"START\",\"TIME-WAIT\",\"EVENT-WAIT\",\"LOCK-WAIT\",\"PREREQ\",\"LOCK\",\"PARAM\",\"RUNNING\"))):\n",
    "            time.sleep(300)\n",
    "            status_load = get_status_loading_id(CTL_URL, loading_id)\n",
    "        if status_load['alive']=='ABORTED':\n",
    "            raise Exception('Error params') # идем к следующему расчету\n",
    "        \n",
    "        elif (status_load['alive']=='ACTIVE' and status_load['status']=='ERROR') or (status_load['alive']=='ACTIVE' and status_load['status']=='ERRORCHECK'):\n",
    "            complete_loading(CTL_URL, loading_id)\n",
    "            raise Exception('Error params') # идем к следующему расчету\n",
    "        \n",
    "        elif status_load['alive']=='COMPLETED' or (status_load['alive']=='ACTIVE' and status_load['status']=='SUCCESS'):\n",
    "            start_dttm = status_load['start_dttm']\n",
    "            end_dttm = status_load['end_dttm']\n",
    "            start_dt = datetime.datetime.strptime(start_dttm[:19], '%Y-%m-%d %H:%M:%S')\n",
    "            end_dt = datetime.datetime.strptime(end_dttm[:19], '%Y-%m-%d %H:%M:%S')\n",
    "        logger.info(f'START date, {start_dt}')\n",
    "        logger.info(f'END date, {end_dt}')\n",
    "        ############# сохранить резы в табличку #############\n",
    "        \n",
    "        if float((end_dt - start_dt).total_seconds()) < min_time:\n",
    "            min_time = float((end_dt - start_dt).total_seconds())\n",
    "            workflow_name = status_load['workflow']['name']\n",
    "            gregor_dt = str(datetime.datetime.now().date())\n",
    "            spark_params_old = custom_param_key\n",
    "            spark_params_new = json.dumps(params)\n",
    "            ctl_datechange = str(datetime.datetime.now())\n",
    "            new_params = [[workflow_name,gregor_dt,min_time,spark_params_old,spark_params_new,loading_id,ctl_datechange,wf_id,key_load]]\n",
    "            logger.info('##########################################')\n",
    "            logger.info(f'Оптимальные параметры! {min_time}')\n",
    "            logger.info(f'Характеристики для потока {wf_id} {workflow_name} и модуля {key_load} за минимальное время {min_time}:\\n {key_load}')\n",
    "            logger.info('##########################################')\n",
    "        else:\n",
    "            logger.info(f'Неоптимальные параметры! {min_time}')\n",
    "        logger.info('%s секунд' % min_time)\n",
    "    df_new_rows = spark.createDataFrame(new_params, df.columns)\n",
    "    df_new_rows.write.partitionBy('workflow_id','model_name').format('hive').mode('append').saveAsTable('spark_params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b0623cd31dae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't_team_mmb_od.spark_params'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "df = spark.table('spark_params')\n",
    "df.show(10,0)\n",
    "\n",
    "#df_new_rows = spark.createDataFrame(new_params, df.columns)\n",
    "#df_new_rows.write.partitionBy('workflow_id','model_name').format('hive').mode('append').saveAsTable('spark_params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS spark_params     (\n",
    "                                               workflow_name STRING COMMENT ''\n",
    "                                              ,gregor_dt TIMESTAMP COMMENT 'дата расчета'\n",
    "                                              ,min_times BIGINT COMMENT ''\n",
    "                                              ,spark_params_old STRING COMMENT ''\n",
    "                                              ,spark_params_new STRING COMMENT ''\n",
    "                                              ,ctl_loading BIGINT COMMENT ''\n",
    "                                              ,ctl_datechange TIMESTAMP COMMENT 'дата расчета'\n",
    "                                      )\n",
    "COMMENT 'таблица технического мониторинга'\n",
    "PARTITIONED BY (    workflow_id BIGINT COMMENT ''\n",
    "                   ,model_name STRING COMMENT ''\n",
    "               )\n",
    "STORED AS PARQUET\n",
    "TBLPROPERTIES ('transactional'='false')\n",
    "\"\"\").show(10,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Расчет с помощью отдельных функций (РУЧНОЕ ТЕСТИРОВАНИЕ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Ym89xzz9wiiH",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f8860ede630>\n",
      "<pyspark.sql.session.SparkSession object at 0x7f8860ede630>\n",
      "+------+-------+-------+--------+----------------+-----------+--------------+-----------------+------------------+----------------+--------+-----------------+----------------+-----------+-----------------+----------------+----------------+----------+--------+---+-------+----------------+-----------+--------------+--------+---------------+--------------+------------------+----------+----------+---------------+----------------+----------------------+-----------------------+-------+------------+---------------------+----------------+---+-----------+---+--------------------+---------------------------+----+----+----+---+---------+-------------------+------------+---------------+-----------------+------------------------+---------------------+-----+-----+-----+------+------+------+------+------+------+---+---+------------------+----------------+-------------------+--------------------------+-------------------+-----------+-------+--------+--------+-----+-------------+----------------------------+---------------------+-----------------------------+--------------+-----------------+-----------------+-------------------+------------------+--------------+--------------------------+------------------+-------------+-----------------------+-----------+----------------------+----------+------------+------------------+---------------------+----------+-------------------+-----------------------+---------+--------------+-----------+----------+---------+\n",
      "|org_id|org_sid|inn_num|ogrn_num|reg_fns_start_dt|init_reg_dt|ogrn_assign_dt|init_reg_egrul_dt|fund_enter_info_dt|min_first_reg_dt|close_dt|region_kladr_code|region_type_code|region_name|msp_list_ctgry_nm|msp_list_incl_dt|msp_list_excl_dt|active_flg|org_name|opf|bus_age|age_egrip_ogrnip|ul_founders|ul_staff_range|bis_segm|report_year_ord|profit_rub_amt|net_assets_rub_amt|net_income|net_assets|revenue_rub_amt|debt_due_rub_amt|cur_fin_invest_rub_amt|other_cur_asset_rub_amt|avg_fca|fund_rub_amt|borrowed_fund_rub_amt|brw_fund_rub_amt|k58|tax_rub_amt|k59|profit_b_tax_rub_amt|borrowed_short_liab_rub_amt|p111|p112|p113|p2 |limit_rmk|max_report_year_ord|cash_rub_amt|acc_pay_rub_amt|long_liab_rub_amt|mobile_cur_asset_rub_amt|own_trnovr_period_qty|k_1_a|k_5_a|k_6_a|k_13_a|k_15_a|k_18_a|k_20_a|k_21_a|k_22_a|k24|k45|autonomy_coeff_qty|asset_profit_qty|k_flexibility_cfcnt|k_stability_current_assets|k_involvement_cfcnt|has_1y_data|k_5_a_1|k_15_a_1|k_23_a_1|k40_1|k_deb_share_1|k_stability_current_assets_1|k_involvement_cfcnt_1|borrowed_short_liab_rub_amt_1|cash_rub_amt_1|acc_pay_rub_amt_1|revenue_rub_amt_1|long_liab_rub_amt_1|debt_due_rub_amt_1|fund_rub_amt_1|mobile_cur_asset_rub_amt_1|brw_fund_rub_amt_1|tax_rub_amt_1|own_trnovr_period_qty_1|has_2y_data|ret_k_turnover_capital|ret_k_15_a|flg_max_year|code_division_main|code_division_main_ul|okved_code|share_nonres_more25|share_nonresident_fndrs|report_dt|ctl_datechange|ctl_loading|ctl_action|gregor_dt|\n",
      "+------+-------+-------+--------+----------------+-----------+--------------+-----------------+------------------+----------------+--------+-----------------+----------------+-----------+-----------------+----------------+----------------+----------+--------+---+-------+----------------+-----------+--------------+--------+---------------+--------------+------------------+----------+----------+---------------+----------------+----------------------+-----------------------+-------+------------+---------------------+----------------+---+-----------+---+--------------------+---------------------------+----+----+----+---+---------+-------------------+------------+---------------+-----------------+------------------------+---------------------+-----+-----+-----+------+------+------+------+------+------+---+---+------------------+----------------+-------------------+--------------------------+-------------------+-----------+-------+--------+--------+-----+-------------+----------------------------+---------------------+-----------------------------+--------------+-----------------+-----------------+-------------------+------------------+--------------+--------------------------+------------------+-------------+-----------------------+-----------+----------------------+----------+------------+------------------+---------------------+----------+-------------------+-----------------------+---------+--------------+-----------+----------+---------+\n",
      "+------+-------+-------+--------+----------------+-----------+--------------+-----------------+------------------+----------------+--------+-----------------+----------------+-----------+-----------------+----------------+----------------+----------+--------+---+-------+----------------+-----------+--------------+--------+---------------+--------------+------------------+----------+----------+---------------+----------------+----------------------+-----------------------+-------+------------+---------------------+----------------+---+-----------+---+--------------------+---------------------------+----+----+----+---+---------+-------------------+------------+---------------+-----------------+------------------------+---------------------+-----+-----+-----+------+------+------+------+------+------+---+---+------------------+----------------+-------------------+--------------------------+-------------------+-----------+-------+--------+--------+-----+-------------+----------------------------+---------------------+-----------------------------+--------------+-----------------+-----------------+-------------------+------------------+--------------+--------------------------+------------------+-------------+-----------------------+-----------+----------------------+----------+------------+------------------+---------------------+----------+-------------------+-----------------------+---------+--------------+-----------+----------+---------+\n",
      "\n",
      "5.229257583618164 секунд\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f87cadd5978>\n",
      "<pyspark.sql.session.SparkSession object at 0x7f87cadd5978>\n",
      "+------+-------+-------+--------+----------------+-----------+--------------+-----------------+------------------+----------------+--------+-----------------+----------------+-----------+-----------------+----------------+----------------+----------+--------+---+-------+----------------+-----------+--------------+--------+---------------+--------------+------------------+----------+----------+---------------+----------------+----------------------+-----------------------+-------+------------+---------------------+----------------+---+-----------+---+--------------------+---------------------------+----+----+----+---+---------+-------------------+------------+---------------+-----------------+------------------------+---------------------+-----+-----+-----+------+------+------+------+------+------+---+---+------------------+----------------+-------------------+--------------------------+-------------------+-----------+-------+--------+--------+-----+-------------+----------------------------+---------------------+-----------------------------+--------------+-----------------+-----------------+-------------------+------------------+--------------+--------------------------+------------------+-------------+-----------------------+-----------+----------------------+----------+------------+------------------+---------------------+----------+-------------------+-----------------------+---------+--------------+-----------+----------+---------+\n",
      "|org_id|org_sid|inn_num|ogrn_num|reg_fns_start_dt|init_reg_dt|ogrn_assign_dt|init_reg_egrul_dt|fund_enter_info_dt|min_first_reg_dt|close_dt|region_kladr_code|region_type_code|region_name|msp_list_ctgry_nm|msp_list_incl_dt|msp_list_excl_dt|active_flg|org_name|opf|bus_age|age_egrip_ogrnip|ul_founders|ul_staff_range|bis_segm|report_year_ord|profit_rub_amt|net_assets_rub_amt|net_income|net_assets|revenue_rub_amt|debt_due_rub_amt|cur_fin_invest_rub_amt|other_cur_asset_rub_amt|avg_fca|fund_rub_amt|borrowed_fund_rub_amt|brw_fund_rub_amt|k58|tax_rub_amt|k59|profit_b_tax_rub_amt|borrowed_short_liab_rub_amt|p111|p112|p113|p2 |limit_rmk|max_report_year_ord|cash_rub_amt|acc_pay_rub_amt|long_liab_rub_amt|mobile_cur_asset_rub_amt|own_trnovr_period_qty|k_1_a|k_5_a|k_6_a|k_13_a|k_15_a|k_18_a|k_20_a|k_21_a|k_22_a|k24|k45|autonomy_coeff_qty|asset_profit_qty|k_flexibility_cfcnt|k_stability_current_assets|k_involvement_cfcnt|has_1y_data|k_5_a_1|k_15_a_1|k_23_a_1|k40_1|k_deb_share_1|k_stability_current_assets_1|k_involvement_cfcnt_1|borrowed_short_liab_rub_amt_1|cash_rub_amt_1|acc_pay_rub_amt_1|revenue_rub_amt_1|long_liab_rub_amt_1|debt_due_rub_amt_1|fund_rub_amt_1|mobile_cur_asset_rub_amt_1|brw_fund_rub_amt_1|tax_rub_amt_1|own_trnovr_period_qty_1|has_2y_data|ret_k_turnover_capital|ret_k_15_a|flg_max_year|code_division_main|code_division_main_ul|okved_code|share_nonres_more25|share_nonresident_fndrs|report_dt|ctl_datechange|ctl_loading|ctl_action|gregor_dt|\n",
      "+------+-------+-------+--------+----------------+-----------+--------------+-----------------+------------------+----------------+--------+-----------------+----------------+-----------+-----------------+----------------+----------------+----------+--------+---+-------+----------------+-----------+--------------+--------+---------------+--------------+------------------+----------+----------+---------------+----------------+----------------------+-----------------------+-------+------------+---------------------+----------------+---+-----------+---+--------------------+---------------------------+----+----+----+---+---------+-------------------+------------+---------------+-----------------+------------------------+---------------------+-----+-----+-----+------+------+------+------+------+------+---+---+------------------+----------------+-------------------+--------------------------+-------------------+-----------+-------+--------+--------+-----+-------------+----------------------------+---------------------+-----------------------------+--------------+-----------------+-----------------+-------------------+------------------+--------------+--------------------------+------------------+-------------+-----------------------+-----------+----------------------+----------+------------+------------------+---------------------+----------+-------------------+-----------------------+---------+--------------+-----------+----------+---------+\n",
      "+------+-------+-------+--------+----------------+-----------+--------------+-----------------+------------------+----------------+--------+-----------------+----------------+-----------+-----------------+----------------+----------------+----------+--------+---+-------+----------------+-----------+--------------+--------+---------------+--------------+------------------+----------+----------+---------------+----------------+----------------------+-----------------------+-------+------------+---------------------+----------------+---+-----------+---+--------------------+---------------------------+----+----+----+---+---------+-------------------+------------+---------------+-----------------+------------------------+---------------------+-----+-----+-----+------+------+------+------+------+------+---+---+------------------+----------------+-------------------+--------------------------+-------------------+-----------+-------+--------+--------+-----+-------------+----------------------------+---------------------+-----------------------------+--------------+-----------------+-----------------+-------------------+------------------+--------------+--------------------------+------------------+-------------+-----------------------+-----------+----------------------+----------+------------+------------------+---------------------+----------+-------------------+-----------------------+---------+--------------+-----------+----------+---------+\n",
      "\n",
      "1.3373937606811523 секунд\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f8860edeef0>\n",
      "<pyspark.sql.session.SparkSession object at 0x7f8860edeef0>\n",
      "+------+-------+-------+--------+----------------+-----------+--------------+-----------------+------------------+----------------+--------+-----------------+----------------+-----------+-----------------+----------------+----------------+----------+--------+---+-------+----------------+-----------+--------------+--------+---------------+--------------+------------------+----------+----------+---------------+----------------+----------------------+-----------------------+-------+------------+---------------------+----------------+---+-----------+---+--------------------+---------------------------+----+----+----+---+---------+-------------------+------------+---------------+-----------------+------------------------+---------------------+-----+-----+-----+------+------+------+------+------+------+---+---+------------------+----------------+-------------------+--------------------------+-------------------+-----------+-------+--------+--------+-----+-------------+----------------------------+---------------------+-----------------------------+--------------+-----------------+-----------------+-------------------+------------------+--------------+--------------------------+------------------+-------------+-----------------------+-----------+----------------------+----------+------------+------------------+---------------------+----------+-------------------+-----------------------+---------+--------------+-----------+----------+---------+\n",
      "|org_id|org_sid|inn_num|ogrn_num|reg_fns_start_dt|init_reg_dt|ogrn_assign_dt|init_reg_egrul_dt|fund_enter_info_dt|min_first_reg_dt|close_dt|region_kladr_code|region_type_code|region_name|msp_list_ctgry_nm|msp_list_incl_dt|msp_list_excl_dt|active_flg|org_name|opf|bus_age|age_egrip_ogrnip|ul_founders|ul_staff_range|bis_segm|report_year_ord|profit_rub_amt|net_assets_rub_amt|net_income|net_assets|revenue_rub_amt|debt_due_rub_amt|cur_fin_invest_rub_amt|other_cur_asset_rub_amt|avg_fca|fund_rub_amt|borrowed_fund_rub_amt|brw_fund_rub_amt|k58|tax_rub_amt|k59|profit_b_tax_rub_amt|borrowed_short_liab_rub_amt|p111|p112|p113|p2 |limit_rmk|max_report_year_ord|cash_rub_amt|acc_pay_rub_amt|long_liab_rub_amt|mobile_cur_asset_rub_amt|own_trnovr_period_qty|k_1_a|k_5_a|k_6_a|k_13_a|k_15_a|k_18_a|k_20_a|k_21_a|k_22_a|k24|k45|autonomy_coeff_qty|asset_profit_qty|k_flexibility_cfcnt|k_stability_current_assets|k_involvement_cfcnt|has_1y_data|k_5_a_1|k_15_a_1|k_23_a_1|k40_1|k_deb_share_1|k_stability_current_assets_1|k_involvement_cfcnt_1|borrowed_short_liab_rub_amt_1|cash_rub_amt_1|acc_pay_rub_amt_1|revenue_rub_amt_1|long_liab_rub_amt_1|debt_due_rub_amt_1|fund_rub_amt_1|mobile_cur_asset_rub_amt_1|brw_fund_rub_amt_1|tax_rub_amt_1|own_trnovr_period_qty_1|has_2y_data|ret_k_turnover_capital|ret_k_15_a|flg_max_year|code_division_main|code_division_main_ul|okved_code|share_nonres_more25|share_nonresident_fndrs|report_dt|ctl_datechange|ctl_loading|ctl_action|gregor_dt|\n",
      "+------+-------+-------+--------+----------------+-----------+--------------+-----------------+------------------+----------------+--------+-----------------+----------------+-----------+-----------------+----------------+----------------+----------+--------+---+-------+----------------+-----------+--------------+--------+---------------+--------------+------------------+----------+----------+---------------+----------------+----------------------+-----------------------+-------+------------+---------------------+----------------+---+-----------+---+--------------------+---------------------------+----+----+----+---+---------+-------------------+------------+---------------+-----------------+------------------------+---------------------+-----+-----+-----+------+------+------+------+------+------+---+---+------------------+----------------+-------------------+--------------------------+-------------------+-----------+-------+--------+--------+-----+-------------+----------------------------+---------------------+-----------------------------+--------------+-----------------+-----------------+-------------------+------------------+--------------+--------------------------+------------------+-------------+-----------------------+-----------+----------------------+----------+------------+------------------+---------------------+----------+-------------------+-----------------------+---------+--------------+-----------+----------+---------+\n",
      "+------+-------+-------+--------+----------------+-----------+--------------+-----------------+------------------+----------------+--------+-----------------+----------------+-----------+-----------------+----------------+----------------+----------+--------+---+-------+----------------+-----------+--------------+--------+---------------+--------------+------------------+----------+----------+---------------+----------------+----------------------+-----------------------+-------+------------+---------------------+----------------+---+-----------+---+--------------------+---------------------------+----+----+----+---+---------+-------------------+------------+---------------+-----------------+------------------------+---------------------+-----+-----+-----+------+------+------+------+------+------+---+---+------------------+----------------+-------------------+--------------------------+-------------------+-----------+-------+--------+--------+-----+-------------+----------------------------+---------------------+-----------------------------+--------------+-----------------+-----------------+-------------------+------------------+--------------+--------------------------+------------------+-------------+-----------------------+-----------+----------------------+----------+------------+------------------+---------------------+----------+-------------------+-----------------------+---------+--------------+-----------+----------+---------+\n",
      "\n",
      "1.2448327541351318 секунд\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f87cadd5780>\n",
      "<pyspark.sql.session.SparkSession object at 0x7f87cadd5780>\n",
      "+------+-------+-------+--------+----------------+-----------+--------------+-----------------+------------------+----------------+--------+-----------------+----------------+-----------+-----------------+----------------+----------------+----------+--------+---+-------+----------------+-----------+--------------+--------+---------------+--------------+------------------+----------+----------+---------------+----------------+----------------------+-----------------------+-------+------------+---------------------+----------------+---+-----------+---+--------------------+---------------------------+----+----+----+---+---------+-------------------+------------+---------------+-----------------+------------------------+---------------------+-----+-----+-----+------+------+------+------+------+------+---+---+------------------+----------------+-------------------+--------------------------+-------------------+-----------+-------+--------+--------+-----+-------------+----------------------------+---------------------+-----------------------------+--------------+-----------------+-----------------+-------------------+------------------+--------------+--------------------------+------------------+-------------+-----------------------+-----------+----------------------+----------+------------+------------------+---------------------+----------+-------------------+-----------------------+---------+--------------+-----------+----------+---------+\n",
      "|org_id|org_sid|inn_num|ogrn_num|reg_fns_start_dt|init_reg_dt|ogrn_assign_dt|init_reg_egrul_dt|fund_enter_info_dt|min_first_reg_dt|close_dt|region_kladr_code|region_type_code|region_name|msp_list_ctgry_nm|msp_list_incl_dt|msp_list_excl_dt|active_flg|org_name|opf|bus_age|age_egrip_ogrnip|ul_founders|ul_staff_range|bis_segm|report_year_ord|profit_rub_amt|net_assets_rub_amt|net_income|net_assets|revenue_rub_amt|debt_due_rub_amt|cur_fin_invest_rub_amt|other_cur_asset_rub_amt|avg_fca|fund_rub_amt|borrowed_fund_rub_amt|brw_fund_rub_amt|k58|tax_rub_amt|k59|profit_b_tax_rub_amt|borrowed_short_liab_rub_amt|p111|p112|p113|p2 |limit_rmk|max_report_year_ord|cash_rub_amt|acc_pay_rub_amt|long_liab_rub_amt|mobile_cur_asset_rub_amt|own_trnovr_period_qty|k_1_a|k_5_a|k_6_a|k_13_a|k_15_a|k_18_a|k_20_a|k_21_a|k_22_a|k24|k45|autonomy_coeff_qty|asset_profit_qty|k_flexibility_cfcnt|k_stability_current_assets|k_involvement_cfcnt|has_1y_data|k_5_a_1|k_15_a_1|k_23_a_1|k40_1|k_deb_share_1|k_stability_current_assets_1|k_involvement_cfcnt_1|borrowed_short_liab_rub_amt_1|cash_rub_amt_1|acc_pay_rub_amt_1|revenue_rub_amt_1|long_liab_rub_amt_1|debt_due_rub_amt_1|fund_rub_amt_1|mobile_cur_asset_rub_amt_1|brw_fund_rub_amt_1|tax_rub_amt_1|own_trnovr_period_qty_1|has_2y_data|ret_k_turnover_capital|ret_k_15_a|flg_max_year|code_division_main|code_division_main_ul|okved_code|share_nonres_more25|share_nonresident_fndrs|report_dt|ctl_datechange|ctl_loading|ctl_action|gregor_dt|\n",
      "+------+-------+-------+--------+----------------+-----------+--------------+-----------------+------------------+----------------+--------+-----------------+----------------+-----------+-----------------+----------------+----------------+----------+--------+---+-------+----------------+-----------+--------------+--------+---------------+--------------+------------------+----------+----------+---------------+----------------+----------------------+-----------------------+-------+------------+---------------------+----------------+---+-----------+---+--------------------+---------------------------+----+----+----+---+---------+-------------------+------------+---------------+-----------------+------------------------+---------------------+-----+-----+-----+------+------+------+------+------+------+---+---+------------------+----------------+-------------------+--------------------------+-------------------+-----------+-------+--------+--------+-----+-------------+----------------------------+---------------------+-----------------------------+--------------+-----------------+-----------------+-------------------+------------------+--------------+--------------------------+------------------+-------------+-----------------------+-----------+----------------------+----------+------------+------------------+---------------------+----------+-------------------+-----------------------+---------+--------------+-----------+----------+---------+\n",
      "+------+-------+-------+--------+----------------+-----------+--------------+-----------------+------------------+----------------+--------+-----------------+----------------+-----------+-----------------+----------------+----------------+----------+--------+---+-------+----------------+-----------+--------------+--------+---------------+--------------+------------------+----------+----------+---------------+----------------+----------------------+-----------------------+-------+------------+---------------------+----------------+---+-----------+---+--------------------+---------------------------+----+----+----+---+---------+-------------------+------------+---------------+-----------------+------------------------+---------------------+-----+-----+-----+------+------+------+------+------+------+---+---+------------------+----------------+-------------------+--------------------------+-------------------+-----------+-------+--------+--------+-----+-------------+----------------------------+---------------------+-----------------------------+--------------+-----------------+-----------------+-------------------+------------------+--------------+--------------------------+------------------+-------------+-----------------------+-----------+----------------------+----------+------------+------------------+---------------------+----------+-------------------+-----------------------+---------+--------------+-----------+----------+---------+\n",
      "\n",
      "1.2426917552947998 секунд\n",
      "Context ready: <pyspark.sql.session.SparkSession object at 0x7f87cadd5cf8>\n",
      "<pyspark.sql.session.SparkSession object at 0x7f87cadd5cf8>\n",
      "+------+-------+-------+--------+----------------+-----------+--------------+-----------------+------------------+----------------+--------+-----------------+----------------+-----------+-----------------+----------------+----------------+----------+--------+---+-------+----------------+-----------+--------------+--------+---------------+--------------+------------------+----------+----------+---------------+----------------+----------------------+-----------------------+-------+------------+---------------------+----------------+---+-----------+---+--------------------+---------------------------+----+----+----+---+---------+-------------------+------------+---------------+-----------------+------------------------+---------------------+-----+-----+-----+------+------+------+------+------+------+---+---+------------------+----------------+-------------------+--------------------------+-------------------+-----------+-------+--------+--------+-----+-------------+----------------------------+---------------------+-----------------------------+--------------+-----------------+-----------------+-------------------+------------------+--------------+--------------------------+------------------+-------------+-----------------------+-----------+----------------------+----------+------------+------------------+---------------------+----------+-------------------+-----------------------+---------+--------------+-----------+----------+---------+\n",
      "|org_id|org_sid|inn_num|ogrn_num|reg_fns_start_dt|init_reg_dt|ogrn_assign_dt|init_reg_egrul_dt|fund_enter_info_dt|min_first_reg_dt|close_dt|region_kladr_code|region_type_code|region_name|msp_list_ctgry_nm|msp_list_incl_dt|msp_list_excl_dt|active_flg|org_name|opf|bus_age|age_egrip_ogrnip|ul_founders|ul_staff_range|bis_segm|report_year_ord|profit_rub_amt|net_assets_rub_amt|net_income|net_assets|revenue_rub_amt|debt_due_rub_amt|cur_fin_invest_rub_amt|other_cur_asset_rub_amt|avg_fca|fund_rub_amt|borrowed_fund_rub_amt|brw_fund_rub_amt|k58|tax_rub_amt|k59|profit_b_tax_rub_amt|borrowed_short_liab_rub_amt|p111|p112|p113|p2 |limit_rmk|max_report_year_ord|cash_rub_amt|acc_pay_rub_amt|long_liab_rub_amt|mobile_cur_asset_rub_amt|own_trnovr_period_qty|k_1_a|k_5_a|k_6_a|k_13_a|k_15_a|k_18_a|k_20_a|k_21_a|k_22_a|k24|k45|autonomy_coeff_qty|asset_profit_qty|k_flexibility_cfcnt|k_stability_current_assets|k_involvement_cfcnt|has_1y_data|k_5_a_1|k_15_a_1|k_23_a_1|k40_1|k_deb_share_1|k_stability_current_assets_1|k_involvement_cfcnt_1|borrowed_short_liab_rub_amt_1|cash_rub_amt_1|acc_pay_rub_amt_1|revenue_rub_amt_1|long_liab_rub_amt_1|debt_due_rub_amt_1|fund_rub_amt_1|mobile_cur_asset_rub_amt_1|brw_fund_rub_amt_1|tax_rub_amt_1|own_trnovr_period_qty_1|has_2y_data|ret_k_turnover_capital|ret_k_15_a|flg_max_year|code_division_main|code_division_main_ul|okved_code|share_nonres_more25|share_nonresident_fndrs|report_dt|ctl_datechange|ctl_loading|ctl_action|gregor_dt|\n",
      "+------+-------+-------+--------+----------------+-----------+--------------+-----------------+------------------+----------------+--------+-----------------+----------------+-----------+-----------------+----------------+----------------+----------+--------+---+-------+----------------+-----------+--------------+--------+---------------+--------------+------------------+----------+----------+---------------+----------------+----------------------+-----------------------+-------+------------+---------------------+----------------+---+-----------+---+--------------------+---------------------------+----+----+----+---+---------+-------------------+------------+---------------+-----------------+------------------------+---------------------+-----+-----+-----+------+------+------+------+------+------+---+---+------------------+----------------+-------------------+--------------------------+-------------------+-----------+-------+--------+--------+-----+-------------+----------------------------+---------------------+-----------------------------+--------------+-----------------+-----------------+-------------------+------------------+--------------+--------------------------+------------------+-------------+-----------------------+-----------+----------------------+----------+------------+------------------+---------------------+----------+-------------------+-----------------------+---------+--------------+-----------+----------+---------+\n",
      "+------+-------+-------+--------+----------------+-----------+--------------+-----------------+------------------+----------------+--------+-----------------+----------------+-----------+-----------------+----------------+----------------+----------+--------+---+-------+----------------+-----------+--------------+--------+---------------+--------------+------------------+----------+----------+---------------+----------------+----------------------+-----------------------+-------+------------+---------------------+----------------+---+-----------+---+--------------------+---------------------------+----+----+----+---+---------+-------------------+------------+---------------+-----------------+------------------------+---------------------+-----+-----+-----+------+------+------+------+------+------+---+---+------------------+----------------+-------------------+--------------------------+-------------------+-----------+-------+--------+--------+-----+-------------+----------------------------+---------------------+-----------------------------+--------------+-----------------+-----------------+-------------------+------------------+--------------+--------------------------+------------------+-------------+-----------------------+-----------+----------------------+----------+------------+------------------+---------------------+----------+-------------------+-----------------------+---------+--------------+-----------+----------+---------+\n",
      "\n",
      "Неоптимальные параметры\n",
      "1.2699742317199707 секунд\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/sdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1152, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/rh/rh-python36/root/lib64/python3.6/socket.py\", line 586, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/usr/sdp/current/spark2-client/python/pyspark/context.py\", line 269, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/usr/sdp/current/spark2-client/python/pyspark/context.py\", line 1039, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "AttributeError: 'NoneType' object has no attribute 'sc'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/sdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/sdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-71f3e277468e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m                             \u001b[0mexecutor_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'spark.executor.memory'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0mmemory_overhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'spark.executor.memoryOverhead'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                             \u001b[0mmemory_fraction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'spark.memory.fraction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0mmemory_storageFraction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'spark.memory.storageFraction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                             shuffle_file_buffer=item['spark.shuffle.file.buffer'],   network_timeout=item['spark.network.timeout'])\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-51b22721a47e>\u001b[0m in \u001b[0;36mstart_spark_session\u001b[0;34m(application_name, type_serializer, kryo_buffer_max, kryo_buffer, executor_cores, driver_memory, executor_memory, memory_overhead, memory_fraction, memory_storageFraction, shuffle_file_buffer, network_timeout)\u001b[0m\n\u001b[1;32m     46\u001b[0m     ])\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yarn-client'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplication_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Context ready: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m#spark.sql('use {}'.format(default_schema));\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/sdp/current/spark2-client/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m                     \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/sdp/current/spark2-client/python/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/sdp/current/spark2-client/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m--> 136\u001b[0;31m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/sdp/current/spark2-client/python/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/sdp/current/spark2-client/python/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \"\"\"\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/sdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1525\u001b[0;31m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[1;32m   1526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/sdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/sdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    334\u001b[0m             raise Py4JError(\n\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 format(target_id, \".\", name))\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext"
     ]
    }
   ],
   "source": [
    "# Модуль обучения (передаем номер потока, который начал дольше считаться) CHECK_SPARK_PARAMS (SPARMS):\n",
    "# запустить каждый модуль потока отдельно, вырубив остальные модули и чекаться на этом\n",
    "\n",
    "# 1) поискать в потоке все модули по параметрам потока, ориентируясь на custom_param\n",
    "# 2) в цикле пройтись и запустить поток с одним включенным модулем и чекать время выполнения\n",
    "# 3) сохранить результаты в таблице:\n",
    "#       Номер потока; название моделя; текущая дата расчета; параметры\n",
    "\n",
    "param_grid = { 'spark.serializer'                : ['org.apache.spark.serializer.KryoSerializer', 'org.apache.spark.serializer.JavaSerializer']    # выбрать одно значение для всех случаев\n",
    "              ,'spark.kryoserializer.buffer.max' : [32, 64, 128, 256]      # выбрать одно значение для всех случаев\n",
    "              ,'spark.kryoserializer.buffer'     : [32, 64, 128, 256]      # выбрать одно значение для всех случаев\n",
    "              ,'spark.executor.cores'            : [2, 3, 4, 5]            # выбрать одно значение для всех случаев\n",
    "              ,'spark.network.timeout'           : [120, 500, 1000, 3000]  # выбрать одно значение для всех случаев\n",
    "              ,'spark.driver.memory'             : [2, 4, 8, 16]\n",
    "              ,'spark.executor.memory'           : [8, 16, 24, 32]\n",
    "              ,'spark.executor.memoryOverhead'   : [1, 2, 4, 8]\n",
    "              ,'spark.memory.fraction'           : [0.6, 0.7, 0.8, 0.9]\n",
    "              ,'spark.memory.storageFraction'    : [0.5, 0.6, 0.7, 0.8]\n",
    "              ,'spark.shuffle.file.buffer'       : [64, 128, 256, 512]\n",
    "              #,'spark.shuffle.manager'           : ['sort']\n",
    "              #,'spark.shuffle.compress'          : [True]\n",
    "              #,'spark.shuffle.spill.compress'    : [True]\n",
    "              }\n",
    "\n",
    "#param_grid_iter = iterator_dict(**param_grid)\n",
    "# самое минимальное и оптимальное время выполнения\n",
    "min_time = 100000000\n",
    "min_params = {} # самое минимальные и оптимальные параметры для выполнения\n",
    "start_calc_fnc = [start_calc_1\n",
    "                 ,start_calc_2\n",
    "                 ,start_calc_3]  # массив функций для вычислений\n",
    "\n",
    "for func in start_calc_fnc:\n",
    "    logger.info(func.__name__)\n",
    "    param_grid_iter = iterator_dict(**param_grid)\n",
    "    min_time = 100000000\n",
    "    for item in param_grid_iter:\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "        spark = start_spark_session(application_name = 'pi pu pa',                   type_serializer=item['spark.serializer'],\n",
    "                            kryo_buffer_max=item['spark.kryoserializer.buffer.max'], kryo_buffer  =  item['spark.kryoserializer.buffer'], \n",
    "                            executor_cores =item['spark.executor.cores'],            driver_memory = item['spark.driver.memory'], \n",
    "                            executor_memory=item['spark.executor.memory'],           memory_overhead=item['spark.executor.memoryOverhead'],\n",
    "                            memory_fraction=item['spark.memory.fraction'],           memory_storageFraction=item['spark.memory.storageFraction'], \n",
    "                            shuffle_file_buffer=item['spark.shuffle.file.buffer'],   network_timeout=item['spark.network.timeout'])\n",
    "        print(spark)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        func() # добавить функцию для запуска потока с определенным модулем\n",
    "        \n",
    "        if (time.time() - start_time) <= min_time:\n",
    "            min_time = (time.time() - start_time)\n",
    "            logger.info('##########################################')\n",
    "            logger.info(f'Оптимальные параметры! {min_time}')\n",
    "            logger.info(\"Характеристики: memory_fraction={}, memory_storageFraction={}, остальные параметры дефолтные\".format(item['spark.memory.fraction'], item['spark.memory.storageFraction']))\n",
    "            logger.info('##########################################')\n",
    "            \n",
    "        else:\n",
    "            logger.info(f'Неоптимальные параметры! {min_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Nnr0q1MwpZHo"
   },
   "outputs": [],
   "source": [
    "# Модуль поиска неоптимальных расчетов CHECK_WF_TIME (WFT)\n",
    "# 1) хранить в таблице по номерам потока историю расчета - сколько времени занимал расчет потока (можно ли получить историю сколько времени занимал расчет каждого модуля)?\n",
    "# Таблица: \n",
    "#      номер потока; текущая дата чека; время начала расчета; время окончания расчета; продолжительность\n",
    "# 2) если потратилось больше времени, чем среднее время расчета за весь период с отклоенением 10%, то запустить перерасчет параметров\n",
    "# 3) получить значения новых параметров после перерасчета и изменить их для каждого модуля у потока, оповестив об этом в почте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "id": "KRioPphcpX1T"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "CHpiEG9qaULI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ggc_v0DaaUPT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
